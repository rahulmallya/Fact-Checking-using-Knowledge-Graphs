{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fact Checking using Knowledge Graphs"
      ],
      "metadata": {
        "id": "oYy_hAeGn_rX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A fact verification system is built which uses knowledge graphs to develop an efficient and accurate engine that can evaluate the authenticity of a given claim based on the available knowledge graph.\n",
        "\n",
        "The system should be able to extract relevant information from the knowledge graphs and use it to verify the claim. The evidence may be supporting or refuting the claims to classify the claim as valid or invalid"
      ],
      "metadata": {
        "id": "53nxvv0zsU2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fact verification engine takes input as claims. Based on the data in the knowledge graph, the model gives an output if the given claim is valid or false, along with evidences which can support or refute the given claim.\n",
        "\n",
        "This is achieved by creating a knowledge graph from the FEVER (Fact Extraction and Verification) dataset, and training a model on the knowledge graph after suitable preprocessing and feature extraction."
      ],
      "metadata": {
        "id": "B_gNr316t9QS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting Drive"
      ],
      "metadata": {
        "id": "sZGNYDZ0rk-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLyXN07ZSbcQ",
        "outputId": "8e2c4461-d912-462f-d557-15ce24704cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link for the dataset:\n",
        "[FEVER dataset](https://drive.google.com/file/d/1oF-L881NDs_Qkqijqeg-13cddDPWzVER/view?usp=sharing)"
      ],
      "metadata": {
        "id": "UVrj11f-rogS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATASET\n",
        "\n",
        "The dataset used is called the FEVER (Fact extraction and Verification) Dataset \n",
        "\n",
        "The FEVER dataset is used for the following task as after performing literature review, it was noticed that no has created a knowledge graph from this dataset.\n",
        "\n",
        "\n",
        "*   It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from.\n",
        "\n",
        "*   The claims are classified as Supported, Refuted or NotEnoughInfo. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment.\n",
        "\n",
        "Dataset Source: [Source to FEVER](https://fever.ai/dataset/fever.html)\n"
      ],
      "metadata": {
        "id": "UwSz5u6zsnLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing necessary packages"
      ],
      "metadata": {
        "id": "-ZMwD-XItw9v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-fzN8HayUwO",
        "outputId": "4f056b97-5960-45fc-9dc7-6bbccc7d70aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.20.1\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (3.12.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.20.1) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.20.1) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.20.1) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.20.1) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ],
      "source": [
        "pip install transformers==4.20.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN0XeGvhq9U6",
        "outputId": "33b963cc-19f0-43e1-e7a7-a7fe9f9d371d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Necessary imports"
      ],
      "metadata": {
        "id": "hEpaUztsYHkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import networkx as nx\n",
        "import math \n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "import torch\n",
        "import torch.nn\n",
        "import seaborn as sns\n",
        "import transformers\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n"
      ],
      "metadata": {
        "id": "i_9GqztAz1Zo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3567996b-3d0a-4779-a830-1f91b4527182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)# to use gpu if possible"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXX8bZ1B0kIw",
        "outputId": "029abf1e-4a88-43cc-9b2e-8f1f78acd350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Traversing the FEVER Dataset to create the knowledge graph"
      ],
      "metadata": {
        "id": "gcvSoKj2wEK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consists of the following columns:\n",
        "* id: The ID of the claim\n",
        "\n",
        "* label: The annotated label for the claim. Can be one of SUPPORTS|REFUTES|NOT ENOUGH INFO.\n",
        "\n",
        "* claim: The text of the claim.\n",
        "\n",
        "* evidence: A list of evidence sets (lists of [Annotation ID, Evidence ID, Wikipedia URL, sentence ID] tuples) or a [Annotation ID, Evidence ID, null, null] tuple if the label is NOT ENOUGH INFO."
      ],
      "metadata": {
        "id": "J74hHbEmww7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have defined the knowledge graph as a networkx graph with the following features:\n",
        "\n",
        "* The nodes of the knowledge graph consist of claims and evidences.\n",
        "\n",
        "* The edge connecting the nodes and evidences contains a label which provides us information whether the evidence “supports” or “refutes” the claim.\n",
        "\n",
        "* Non verifiable claims (not enough info) have been ommited while creating the KG.\n"
      ],
      "metadata": {
        "id": "vcxxRpsRw0R6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the JSONL file and parse the data\n",
        "def read_fever_jsonl(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "# Create the knowledge graph\n",
        "def create_knowledge_graph(data):\n",
        "    kg = nx.DiGraph()\n",
        "    \n",
        "    for item in data:\n",
        "        claim_id = item['id']\n",
        "        claim_text = item['claim']\n",
        "        label = item['label']\n",
        "        \n",
        "        # Add claim node\n",
        "        kg.add_node(claim_id, label=\"claim\", text=claim_text)\n",
        "        \n",
        "        if label != \"NOT ENOUGH INFO\":\n",
        "            for evidence_group in item['evidence']:\n",
        "                for evidence in evidence_group:\n",
        "                    evidence_id = evidence[1]\n",
        "                    evidence_title = evidence[2]\n",
        "                    evidence_sentence_num = evidence[3]\n",
        "                    \n",
        "                    # Add evidence node\n",
        "                    kg.add_node(evidence_id, label=\"evidence\", title=evidence_title, sentence_num=evidence_sentence_num)\n",
        "                    \n",
        "                    # Add edge between claim and evidence with the relationship label\n",
        "                    kg.add_edge(claim_id, evidence_id, label=label)\n",
        "    \n",
        "    return kg"
      ],
      "metadata": {
        "id": "H4sJTSBZ01VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/NAM/train.jsonl\"\n",
        "data = read_fever_jsonl(file_path)\n",
        "knowledge_graph = create_knowledge_graph(data)"
      ],
      "metadata": {
        "id": "jvy0VVP-xf8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "b5xcFCBoxuAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total number of nodes and edges in the knowledge graph"
      ],
      "metadata": {
        "id": "I_mFvQobx_OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Nodes in the knowledge graph:\", knowledge_graph.number_of_nodes())\n",
        "print(\"Edges in the knowledge graph:\", knowledge_graph.number_of_edges())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRaMhEXu1C84",
        "outputId": "024b1722-b084-40f9-cc58-eb9bcc8d3364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nodes in the knowledge graph: 268910\n",
            "Edges in the knowledge graph: 221476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Total number of edges with \"REFUTES\" label"
      ],
      "metadata": {
        "id": "nZ9i6wSlyFAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_refutes_edges(kg):\n",
        "    count = 0\n",
        "    for _, _, edge_data in kg.edges(data=True):\n",
        "        if edge_data['label'] == 'REFUTES':\n",
        "          count+=1\n",
        "    return count\n",
        "    \n",
        "refutes_count = count_refutes_edges(knowledge_graph)\n",
        "print(\"Number of nodes with a REFUTES relationship:\", refutes_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXI5VUIox3t1",
        "outputId": "f980f3d7-9675-4001-81fc-b64c71a9a22a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes with a REFUTES relationship: 60227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Total number of edges with \"SUPPORTS\" label"
      ],
      "metadata": {
        "id": "pO5aLhcsyNSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_supports_edges(kg):\n",
        "    count = 0\n",
        "    for _, _, edge_data in kg.edges(data=True):\n",
        "        if edge_data['label'] == 'SUPPORTS':\n",
        "          count+=1\n",
        "    return count\n",
        "    \n",
        "\n",
        "supports_count = count_supports_edges(knowledge_graph)\n",
        "print(\"Number of nodes with a SUPPORTS relationship:\", supports_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhuOB-2bx6xC",
        "outputId": "d78c6ba8-89bc-446e-95b5-e6d1a5ab46ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes with a SUPPORTS relationship: 161249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "JKBdG59NzNCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following preprocessing steps have been taken into account:\n",
        "\n",
        "* Tokenization\n",
        "* Removing stop words\n",
        "* Lemmatization"
      ],
      "metadata": {
        "id": "Z6yqmTb9zR55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return ' '.join(tokens)\n"
      ],
      "metadata": {
        "id": "5UmPJzd-TENZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding top-k evidences for a given claim"
      ],
      "metadata": {
        "id": "FoPU0julzt5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_top_k_evidences(kg, claim_id, k=5):\n",
        "    edges = [(evidence_id, data['label']) for _, evidence_id, data in kg.out_edges(claim_id, data=True)]\n",
        "    evidences = sorted(edges, key=lambda x: x[1], reverse=True)[:k]\n",
        "    return evidences\n"
      ],
      "metadata": {
        "id": "_47rPsHDTFP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(kg):\n",
        "    dataset = []\n",
        "    for claim_id, data in kg.nodes(data=True):\n",
        "        if data['label'] == 'claim':\n",
        "            claim_text = preprocess_text(data['text'])\n",
        "            evidences = find_top_k_evidences(kg, claim_id)\n",
        "            for evidence_id, relationship in evidences:\n",
        "                evidence_data = kg.nodes[evidence_id]\n",
        "                evidence_text = preprocess_text(evidence_data['title'])\n",
        "                dataset.append((claim_text, evidence_text, relationship))\n",
        "    return dataset\n",
        "\n",
        "dataset = prepare_dataset(knowledge_graph)\n"
      ],
      "metadata": {
        "id": "aPvwgoTNTI4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Extraction"
      ],
      "metadata": {
        "id": "aBOqklpG1Dw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(dataset):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    claims, evidences, labels = zip(*dataset)\n",
        "    claim_features = vectorizer.fit_transform(claims)\n",
        "    evidence_features = vectorizer.transform(evidences)\n",
        "    return claim_features, evidence_features, labels, vectorizer\n",
        "\n",
        "claim_features, evidence_features, labels, vectorizer = extract_features(dataset)\n"
      ],
      "metadata": {
        "id": "QrwLMml2TUe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dividing training and testing sets"
      ],
      "metadata": {
        "id": "XiyojxfJ2pPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = hstack([claim_features, evidence_features])\n",
        "y = [1 if label == \"SUPPORTS\" else 0 for label in labels]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "4VOyYRX8TW1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "r3Q0hLzc2yvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "6lXRgEe-2wA1",
        "outputId": "2eed84c8-14d4-4fdb-aaec-416f9247b4ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy metrics with the testing set"
      ],
      "metadata": {
        "id": "9On1bY-s26w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icmzxrHGTbfC",
        "outputId": "9931d64d-20e4-4b4e-f4b0-c95f2d8d273f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8110532407407407\n",
            "Precision: 0.8108741504569955\n",
            "Recall: 0.9699484189280108\n",
            "F1 Score: 0.8833065277884148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting claim veracity function"
      ],
      "metadata": {
        "id": "u1S-6d9TWunQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_claim_veracity(claim_text, kg, model, vectorizer):\n",
        "    claim_id = claims(knowledge_graph, claim_text)\n",
        "    if claim_id is None:\n",
        "        print(\"Claim not found in the knowledge graph.\")\n",
        "        return None\n",
        "\n",
        "    claim = preprocess_text(claim_text)\n",
        "    print(\"Claim:\", claim)\n",
        "    claim_features = vectorizer.transform([claim])\n",
        "    #print(\"Claim features shape:\", claim_features.shape)\n",
        "\n",
        "    top_k_evidences = find_top_k_evidences(kg, claim_id)\n",
        "    predictions = []\n",
        "    for evidence_id, relationship in top_k_evidences:\n",
        "        evidence_data = kg.nodes[evidence_id]\n",
        "        evidence_text = evidence_data['title']\n",
        "        #evidence_text = preprocess_text(evidence_data['title'])\n",
        "        # print(\"Evidence text:\", evidence_text)\n",
        "        evidence_features = vectorizer.transform([evidence_text])\n",
        "        #print(\"Evidence features shape:\", evidence_features.shape)\n",
        "        features = hstack([claim_features, evidence_features])\n",
        "        # print(\"Features shape:\", features.shape)\n",
        "\n",
        "        prediction = model.predict(features)\n",
        "        probability = model.predict_proba(features)\n",
        "\n",
        "        if prediction[0] == 1:\n",
        "            predicted_relationship = \"SUPPORTS\"\n",
        "        else:\n",
        "            predicted_relationship = \"REFUTES\"\n",
        "\n",
        "        confidence = max(probability[0])\n",
        "\n",
        "        predictions.append((predicted_relationship, evidence_text, confidence))\n",
        "        #print(\"Prediction:\", prediction)\n",
        "        print(\"Probability:\", probability)\n",
        "\n",
        "    return predictions, top_k_evidences\n",
        "\n",
        "def claims(kg, claim_text):\n",
        "    for node_id, data in knowledge_graph.nodes(data=True):\n",
        "        if data['label'] == 'claim' and data['text'] == claim_text:\n",
        "            return node_id\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "YfGi0XdxSUGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main fact checking function"
      ],
      "metadata": {
        "id": "nQpBBY9RWoPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fact Checking function\n",
        "def fact_check(claim_text):\n",
        "    predictions, top_k_evidences = predict_claim_veracity(claim_text, knowledge_graph, model, vectorizer)\n",
        "    if predictions is not None:\n",
        "        supports_count = 0\n",
        "        refutes_count = 0\n",
        "        for predicted_relationship, evidence_text, confidence in predictions:\n",
        "            print(f\"{predicted_relationship} with evidence: {evidence_text} and confidence: {confidence:.2f}\")\n",
        "            if predicted_relationship == \"SUPPORTS\":\n",
        "                supports_count += 1\n",
        "            elif predicted_relationship == \"REFUTES\":\n",
        "                refutes_count += 1\n",
        "        if supports_count > refutes_count:\n",
        "            print(\"Claim is valid.\")\n",
        "        else:\n",
        "            print(\"Claim is invalid.\")  \n",
        "    else:\n",
        "        print(\"No predictions were made for the claim.\")\n",
        " \n",
        " "
      ],
      "metadata": {
        "id": "TDphB4UhACdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working"
      ],
      "metadata": {
        "id": "1ciEBWKgaBY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, while running the project with some test claims to demonstrate the working of the project,\n",
        "\n",
        "For every claim provided, the outputs are in the format of:\n",
        "\n",
        "*   Claim\n",
        "*   Probability[invalid claim, valid claim]\n",
        "* Evidences to support/refute claim along with confidence\n",
        "* Conclusion\n",
        "\n"
      ],
      "metadata": {
        "id": "-vM21rVMZnJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Claim-1: Tetris has sold millions of physical copies."
      ],
      "metadata": {
        "id": "4FDZ70Rn4tVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fact_check(\"Tetris has sold millions of physical copies.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxVxbDvgQZmo",
        "outputId": "28801366-2223-4ea1-d21f-2615b89f5da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claim: tetri sold million physical copy\n",
            "Probability: [[0.25977798 0.74022202]]\n",
            "SUPPORTS with evidence: Tetris and confidence: 0.74\n",
            "Claim is valid.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Claim-2: Ariana Grande never lent her voice to animated television and films."
      ],
      "metadata": {
        "id": "tsSifAac4v2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fact_check(\"Ariana Grande never lent her voice to animated television and films.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z47Bb7cAR0U5",
        "outputId": "ee4d1659-bb16-4b7c-8a17-78268d73ff63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claim: ariana grande never lent voice animated television film\n",
            "Probability: [[0.73749738 0.26250262]]\n",
            "REFUTES with evidence: Ariana_Grande and confidence: 0.74\n",
            "Claim is invalid.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Claim-3: London is the location of zero enclaves."
      ],
      "metadata": {
        "id": "ZVsUDMK74uZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fact_check(\"Roman Atwood is a content creator.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nGX8rMyRrtP",
        "outputId": "aa49937c-dca2-4e1c-bd8a-97e4ed5ccf21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claim: roman atwood content creator\n",
            "Probability: [[0.24535581 0.75464419]]\n",
            "Probability: [[0.24535581 0.75464419]]\n",
            "SUPPORTS with evidence: Roman_Atwood and confidence: 0.75\n",
            "SUPPORTS with evidence: Roman_Atwood and confidence: 0.75\n",
            "Claim is valid.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Claim-4: London is the location of zero enclaves."
      ],
      "metadata": {
        "id": "ZOUeCJho4jTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fact_check(\"London is the location of zero enclaves.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTrJQFJ4V33p",
        "outputId": "bf513ded-84c0-4390-fa68-e989b35578c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claim: london location zero enclave\n",
            "Probability: [[0.78579529 0.21420471]]\n",
            "REFUTES with evidence: City_of_London and confidence: 0.79\n",
            "Claim is invalid.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building such as the above fact verification using knowledge graphs has many real-world applications, such as detecting fake news, identifying misinformation, and verifying claims in legal or political contexts. These applications can help to promote transparency, accountability, and trust in public discourse."
      ],
      "metadata": {
        "id": "mWl3wDvUZeAD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GGhrtezixHAu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}